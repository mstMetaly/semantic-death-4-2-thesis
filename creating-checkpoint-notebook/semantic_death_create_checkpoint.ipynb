{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Minimal Pipeline Notebook\n",
        "\n",
        "Use only the cells labeled **PIPELINE** below. Legacy cells from earlier experiments are kept for reference but are not needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz7Dk9v3LDmW"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# ONE-CELL COLAB SETUP\n",
        "# =========================\n",
        "\n",
        "# --------- INSTALLS ---------\n",
        "!pip install -q torch torchvision matplotlib seaborn\n",
        "!git clone -q https://github.com/CSAILVision/NetDissect.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rWCdBJ2eLGna"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --------- IMPORTS ---------\n",
        "import os, random, torch, torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import Places365\n",
        "from torch.utils.data import DataLoader, Subset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdYM5KFuLO0V",
        "outputId": "41befa05-582d-47f3-f233-1f8b86e4e7bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --------- DEVICE ---------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "BPxYFi1aLncm",
        "outputId": "4837f5b3-d08c-436f-b0b3-17dbebf8ac48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Places365 validation downloaded\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n!wget http://data.csail.mit.edu/places/places365/train_256_places365standard.tar\\n!tar -xf train_256_places365standard.tar -C /content/places365\\n'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# --------- DOWNLOAD PLACES365 (SAFE VERSION) ---------\n",
        "os.makedirs(\"/content/places365\", exist_ok=True)\n",
        "\n",
        "# Validation set (RECOMMENDED – small & safe)\n",
        "if not os.path.exists(\"/content/places365/val\"):\n",
        "    !wget -q http://data.csail.mit.edu/places/places365/val_256.tar\n",
        "    !tar -xf val_256.tar -C /content/places365\n",
        "    print(\"Places365 validation downloaded\")\n",
        "\n",
        "# ❗ OPTIONAL: Training data (≈24GB, COMMENTED for safety)\n",
        "# Uncomment ONLY if you have enough disk space\n",
        "\"\"\"\n",
        "!wget http://data.csail.mit.edu/places/places365/train_256_places365standard.tar\n",
        "!tar -xf train_256_places365standard.tar -C /content/places365\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_0qynKyRZVq",
        "outputId": "f4f1afa7-7791-4afb-9bfa-4ad66f11d807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Places365 metadata downloaded\n"
          ]
        }
      ],
      "source": [
        "# ---- FIX: Download required Places365 metadata ----\n",
        "import os\n",
        "\n",
        "os.makedirs(\"/content/places365\", exist_ok=True)\n",
        "\n",
        "!wget -q http://data.csail.mit.edu/places/places365/categories_places365.txt -P /content/places365\n",
        "!wget -q http://data.csail.mit.edu/places/places365/filelist_places365-standard.tar -P /content/places365\n",
        "!tar -xf /content/places365/filelist_places365-standard.tar -C /content/places365\n",
        "\n",
        "print(\"Places365 metadata downloaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-ew8v5pJLWV4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --------- TRANSFORMS ---------\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "g5pw_pksLjLs"
      },
      "outputs": [],
      "source": [
        "# --------- DATASETS (COLAB-SAFE OPTION) ---------\n",
        "\n",
        "# Use validation set for both training and validation\n",
        "train_dataset = Places365(\n",
        "    root=\"/content/places365\",\n",
        "    split=\"val\",\n",
        "    small=True,\n",
        "    download=False,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "val_dataset = Places365(\n",
        "    root=\"/content/places365\",\n",
        "    split=\"val\",\n",
        "    small=True,\n",
        "    download=False,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGZcmFCSLfbF",
        "outputId": "464ffd4b-1b4e-433d-82ab-0e3ef6bd3779"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples: 20000\n",
            "Val samples: 36500\n"
          ]
        }
      ],
      "source": [
        "# Subset for faster training\n",
        "subset_size = min(20000, len(train_dataset))\n",
        "indices = random.sample(range(len(train_dataset)), subset_size)\n",
        "train_subset = Subset(train_dataset, indices)\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Train samples: {len(train_subset)}\")\n",
        "print(f\"Val samples: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ny0YA-xLcqx",
        "outputId": "431937a6-6504-40a1-d190-eae29393b2b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 197MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded: ResNet-18 (ImageNet → Places365)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --------- LOAD PRETRAINED MODEL ---------\n",
        "model = torchvision.models.resnet18(pretrained=True)\n",
        "model.fc = nn.Linear(model.fc.in_features, 365)  # Places365 classes\n",
        "model = model.to(device)\n",
        "\n",
        "print(\"Model loaded: ResNet-18 (ImageNet → Places365)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzw7YeOtLXQc",
        "outputId": "152af533-7dc1-47a6-f666-c07551d725aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Val Acc: 0.2450 | Checkpoint saved\n",
            "Epoch 1 | Val Acc: 0.3260 | Checkpoint saved\n",
            "Epoch 2 | Val Acc: 0.3658 | Checkpoint saved\n",
            "Epoch 3 | Val Acc: 0.4427 | Checkpoint saved\n",
            "Epoch 4 | Val Acc: 0.5081 | Checkpoint saved\n",
            "Training complete.\n",
            "Validation accuracy log: {0: 0.24498630136986302, 1: 0.32602739726027397, 2: 0.3658082191780822, 3: 0.4426849315068493, 4: 0.5081369863013698}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --------- OPTIMIZER & LOSS ---------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# --------- TRAIN & SAVE CHECKPOINTS ---------\n",
        "num_epochs = 5   # start small; increase later\n",
        "\n",
        "val_acc_log = {}\n",
        "\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return correct / total\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(imgs), labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    val_acc = evaluate(model)\n",
        "    val_acc_log[epoch] = val_acc\n",
        "\n",
        "    torch.save(model.state_dict(), f\"/content/checkpoint_epoch_{epoch}.pth\")\n",
        "    print(f\"Epoch {epoch} | Val Acc: {val_acc:.4f} | Checkpoint saved\")\n",
        "\n",
        "print(\"Training complete.\")\n",
        "print(\"Validation accuracy log:\", val_acc_log)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fctIG3DwTqMi"
      },
      "source": [
        "<h2>uploading the checkpoints to drive</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1TIY-7XTUq8",
        "outputId": "d1dd73a2-78ff-4670-d96e-fb9b1ae55f15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgDtxCz2TVhn",
        "outputId": "5eb16e37-327f-4863-b173-794e74e1b534"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint directory: /content/drive/MyDrive/semantic_mortality_checkpoints\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "ckpt_dir = \"/content/drive/MyDrive/semantic_mortality_checkpoints\"\n",
        "os.makedirs(ckpt_dir, exist_ok=True)\n",
        "\n",
        "print(\"Checkpoint directory:\", ckpt_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Sk3SWKdTYAv",
        "outputId": "ca4d455a-1252-4390-cd98-a03d2a2e7f5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All checkpoints copied to Google Drive.\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "for f in os.listdir(\"/content\"):\n",
        "    if f.startswith(\"checkpoint_epoch_\") and f.endswith(\".pth\"):\n",
        "        src = os.path.join(\"/content\", f)\n",
        "        dst = os.path.join(ckpt_dir, f)\n",
        "        shutil.copy(src, dst)\n",
        "\n",
        "print(\"All checkpoints copied to Google Drive.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wI5oj4XdTahs",
        "outputId": "4876504f-3d1e-4330-f28e-ccf696fe1c51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation logs saved.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "with open(\"/content/val_acc_log.json\", \"w\") as f:\n",
        "    json.dump(val_acc_log, f)\n",
        "\n",
        "shutil.copy(\"/content/val_acc_log.json\", ckpt_dir)\n",
        "print(\"Validation logs saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqFoCWRETl1U"
      },
      "source": [
        "<h2>how use them later </h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLNuaQ6XTjAI"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "ckpt_dir = \"/content/drive/MyDrive/semantic_mortality_checkpoints\"\n",
        "print(os.listdir(ckpt_dir))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3o2V7s8TlHg"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(\n",
        "    torch.load(f\"{ckpt_dir}/checkpoint_epoch_5.pth\", map_location=device)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Semantic Mortality Pipeline (Colab)\n",
        "\n",
        "This section runs the end-to-end pipeline: training + checkpoints, NetDissect per checkpoint, tracking/analysis, and plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- PIPELINE MODE ----\n",
        "# quick: small run for sanity check\n",
        "# full: full Places365 training (requires train_256 download)\n",
        "RUN_MODE = \"quick\"  # change to \"full\" when ready"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- REPO SETUP ----\n",
        "# Option A: mount your Drive and point to the repo\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# REPO_DIR = \"/content/drive/MyDrive/Semantic-Death\"\n",
        "\n",
        "# Option B: clone from GitHub\n",
        "REPO_URL = \"https://github.com/mstMetaly/semantic-death-4-2-thesis.git\"\n",
        "REPO_DIR = \"/content/semantic-death-4-2-thesis\"\n",
        "\n",
        "import os\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    !git clone -q {REPO_URL} {REPO_DIR}\n",
        "else:\n",
        "    !git -C {REPO_DIR} pull -q\n",
        "\n",
        "%cd {REPO_DIR}\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -q torch torchvision matplotlib seaborn scikit-image imageio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- PLACES365 DATASET ----\n",
        "# Download Places365 (small). You can switch to full train later.\n",
        "import os\n",
        "os.makedirs(\"/content/places365\", exist_ok=True)\n",
        "\n",
        "if not os.path.exists(\"/content/places365/val\"):\n",
        "    !wget -q http://data.csail.mit.edu/places/places365/val_256.tar\n",
        "    !tar -xf val_256.tar -C /content/places365\n",
        "\n",
        "if not os.path.exists(\"/content/places365/categories_places365.txt\"):\n",
        "    !wget -q http://data.csail.mit.edu/places/places365/categories_places365.txt -P /content/places365\n",
        "\n",
        "# Optional: download training set (large)\n",
        "# !wget http://data.csail.mit.edu/places/places365/train_256_places365standard.tar\n",
        "# !tar -xf train_256_places365standard.tar -C /content/places365\n",
        "\n",
        "print(\"Places365 ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- PIPELINE: TRAIN + CHECKPOINT ----\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "CONFIG_PATH = f\"{REPO_DIR}/configs/places365_resnet18.json\"\n",
        "\n",
        "cfg_path = Path(CONFIG_PATH)\n",
        "if not cfg_path.exists():\n",
        "    raise FileNotFoundError(f\"Config not found: {cfg_path}. Make sure GitHub repo is updated and re-cloned.\")\n",
        "\n",
        "cfg = json.loads(cfg_path.read_text())\n",
        "cfg[\"data\"][\"dataset_root\"] = \"/content/places365\"\n",
        "cfg[\"data\"][\"download\"] = True\n",
        "\n",
        "if RUN_MODE == \"quick\":\n",
        "    cfg[\"data\"][\"train_split\"] = \"val\"\n",
        "    cfg[\"data\"][\"small\"] = True\n",
        "    cfg[\"data\"][\"max_train_samples\"] = 20000\n",
        "    cfg[\"training\"][\"epochs\"] = 5\n",
        "elif RUN_MODE == \"full\":\n",
        "    cfg[\"data\"][\"train_split\"] = \"train-standard\"\n",
        "    cfg[\"data\"][\"small\"] = False\n",
        "    cfg[\"data\"][\"max_train_samples\"] = None\n",
        "    cfg[\"training\"][\"epochs\"] = 90\n",
        "\n",
        "cfg[\"netdissect\"][\"root\"] = str(Path(REPO_DIR) / \"NetDissect-Lite\")\n",
        "\n",
        "cfg_path.write_text(json.dumps(cfg, indent=2))\n",
        "\n",
        "# Patch train.py in Colab if needed to respect download flag\n",
        "train_path = Path(REPO_DIR) / \"semantic_mortality\" / \"train.py\"\n",
        "if train_path.exists():\n",
        "    text = train_path.read_text()\n",
        "    if \"download=False\" in text and \"download = data_cfg.get\" not in text:\n",
        "        text = text.replace(\"download=False\", \"download=download\")\n",
        "        text = text.replace(\"val_tf = transforms.Compose\", \"download = data_cfg.get(\\\"download\\\", False)\\n    val_tf = transforms.Compose\")\n",
        "        train_path.write_text(text)\n",
        "\n",
        "%cd {REPO_DIR}\n",
        "!python -m semantic_mortality.pipeline --stage train --config {CONFIG_PATH}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- NETDISSECT DATA (BRODEN) ----\n",
        "# Download Broden dataset required by NetDissect-Lite\n",
        "%cd {REPO_DIR}/NetDissect-Lite\n",
        "!bash script/dlbroden.sh\n",
        "%cd {REPO_DIR}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- NETDISSECT PER CHECKPOINT ----\n",
        "%cd {REPO_DIR}\n",
        "\n",
        "# Ensure Broden index exists before running\n",
        "import os\n",
        "from pathlib import Path\n",
        "broden_index = f\"{REPO_DIR}/NetDissect-Lite/dataset/broden1_224/index.csv\"\n",
        "if not os.path.exists(broden_index):\n",
        "    %cd {REPO_DIR}/NetDissect-Lite\n",
        "    !bash script/dlbroden.sh\n",
        "    %cd {REPO_DIR}\n",
        "\n",
        "# Force NetDissect settings to point to the correct Broden path\n",
        "settings_path = Path(f\"{REPO_DIR}/NetDissect-Lite/settings.py\")\n",
        "if settings_path.exists():\n",
        "    text = settings_path.read_text()\n",
        "    text = text.replace(\"DATA_DIRECTORY = 'dataset/broden1_224'\", f\"DATA_DIRECTORY = '{REPO_DIR}/NetDissect-Lite/dataset/broden1_224'\")\n",
        "    text = text.replace(\"INDEX_FILE = 'index_sm.csv'\", \"INDEX_FILE = 'index.csv'\")\n",
        "    text = text.replace(\"DATASET = 'imagenet'\", \"DATASET = 'places365'\")\n",
        "    text = text.replace(\"NUM_CLASSES = 1000\", \"NUM_CLASSES = 365\")\n",
        "    settings_path.write_text(text)\n",
        "\n",
        "# Patch model_loader OrderedDict bug for Colab torch build\n",
        "loader_path = Path(f\"{REPO_DIR}/NetDissect-Lite/loader/model_loader.py\")\n",
        "if loader_path.exists():\n",
        "    text = loader_path.read_text()\n",
        "    if \"torch._C.OrderedDict\" in text:\n",
        "        text = text.replace(\"import os\", \"import os\\nfrom collections import OrderedDict\")\n",
        "        text = text.replace(\"type(torch._C.OrderedDict())\", \"OrderedDict\")\n",
        "        loader_path.write_text(text)\n",
        "\n",
        "# Run dissection for a single checkpoint (manual)\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "EPOCH = 0  # set this manually\n",
        "\n",
        "print(f\"Running NetDissect for epoch {EPOCH}\")\n",
        "code = f\"\"\"\n",
        "from pathlib import Path\n",
        "from semantic_mortality.config import load_config, resolve_run_paths\n",
        "from semantic_mortality.netdissect_runner import run_netdissect_per_checkpoint\n",
        "\n",
        "cfg = load_config(Path(r\\\"{CONFIG_PATH}\\\"))\n",
        "paths = resolve_run_paths(Path('runs'), cfg.run_name)\n",
        "run_netdissect_per_checkpoint(cfg, paths, epochs=[{EPOCH}])\n",
        "\"\"\"\n",
        "# Stream output so you can see progress\n",
        "import os\n",
        "proc = subprocess.Popen(\n",
        "    [sys.executable, \"-u\", \"-c\", code],\n",
        "    cwd=REPO_DIR,\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True,\n",
        "    env={**os.environ, \"PYTHONUNBUFFERED\": \"1\"},\n",
        ")\n",
        "for line in proc.stdout:\n",
        "    print(line, end=\"\")\n",
        "ret = proc.wait()\n",
        "if ret != 0:\n",
        "    raise RuntimeError(\"NetDissect subprocess failed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- NETDISSECT SINGLE EPOCH ----\n",
        "# Set which epoch you want to run manually\n",
        "EPOCH = 0\n",
        "\n",
        "from pathlib import Path\n",
        "from semantic_mortality.config import load_config, resolve_run_paths\n",
        "from semantic_mortality.netdissect_runner import run_netdissect_per_checkpoint\n",
        "\n",
        "cfg = load_config(Path(CONFIG_PATH))\n",
        "paths = resolve_run_paths(Path(\"runs\"), cfg.run_name)\n",
        "run_netdissect_per_checkpoint(cfg, paths, epochs=[EPOCH])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- SAVE RUNS TO DRIVE ----\n",
        "# Mount Drive and copy runs/ so results persist across sessions\n",
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "RUNS_DIR = f\"{REPO_DIR}/runs\"\n",
        "STAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "DEST_DIR = f\"/content/drive/MyDrive/semantic_mortality_runs_{STAMP}\"\n",
        "\n",
        "if os.path.exists(RUNS_DIR):\n",
        "    shutil.copytree(RUNS_DIR, DEST_DIR, dirs_exist_ok=True)\n",
        "    print(\"Saved runs to:\", DEST_DIR)\n",
        "else:\n",
        "    print(\"runs/ not found. Make sure training/dissection finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- DOWNLOAD RUNS (LOCAL) ----\n",
        "# Zip runs/ and download to your local machine\n",
        "import os\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "\n",
        "RUNS_DIR = f\"{REPO_DIR}/runs\"\n",
        "STAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "ARCHIVE_BASE = f\"/content/runs_{STAMP}\"\n",
        "ARCHIVE_PATH = shutil.make_archive(ARCHIVE_BASE, 'zip', RUNS_DIR)\n",
        "\n",
        "print(\"Created:\", ARCHIVE_PATH)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(ARCHIVE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- TRACKING + ANALYSIS + PLOTS ----\n",
        "%cd {REPO_DIR}\n",
        "!python -m semantic_mortality.pipeline --stage track --config {CONFIG_PATH}\n",
        "!python -m semantic_mortality.pipeline --stage analyze --config {CONFIG_PATH}\n",
        "!python -m semantic_mortality.pipeline --stage plot --config {CONFIG_PATH}\n",
        "\n",
        "print(\"Done. Check runs/places365_resnet18/analysis and runs/places365_resnet18/plots\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- SANITY CHECK: MORTALITY FROM EPOCH 0-2 ----\n",
        "# Uses the small trajectories file you already have\n",
        "from pathlib import Path\n",
        "from semantic_mortality.mortality import detect_semantic_mortality, write_events\n",
        "\n",
        "trajectories_path = Path(\"ND-ON-SD/results/trajectories_epoch_0-2.csv\")\n",
        "output_path = Path(\"ND-ON-SD/results/mortality_events_epoch_0-2.csv\")\n",
        "\n",
        "if not trajectories_path.exists():\n",
        "    raise FileNotFoundError(f\"Missing: {trajectories_path}\")\n",
        "\n",
        "# Use a low k since we only have 3 epochs here\n",
        "events = detect_semantic_mortality(\n",
        "    trajectories_path=trajectories_path,\n",
        "    tau=0.04,\n",
        "    k=1,\n",
        "    min_alive_epochs=1,\n",
        "    smoothing_window=1,\n",
        ")\n",
        "\n",
        "write_events(events, output_path)\n",
        "print(\"Events found:\", len(events))\n",
        "print(\"Saved:\", output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- COUNT INTERPRETABLE UNITS PER EPOCH ----\n",
        "# Counts rows in tally.csv with score >= tau for each epoch\n",
        "import csv\n",
        "from pathlib import Path\n",
        "\n",
        "TAU = 0.04\n",
        "RUN_DIR = Path(f\"{REPO_DIR}/runs/places365_resnet18/dissection\")\n",
        "\n",
        "if not RUN_DIR.exists():\n",
        "    raise FileNotFoundError(f\"Missing: {RUN_DIR}\")\n",
        "\n",
        "def count_interpretable(tally_path: Path, tau: float) -> int:\n",
        "    count = 0\n",
        "    with tally_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            if float(row.get(\"score\", 0)) >= tau:\n",
        "                count += 1\n",
        "    return count\n",
        "\n",
        "results = []\n",
        "for epoch_dir in sorted(RUN_DIR.glob(\"epoch_*\")):\n",
        "    tally_path = epoch_dir / \"tally.csv\"\n",
        "    if not tally_path.exists():\n",
        "        # Some runs store tally under layer subfolder\n",
        "        layer_tally = epoch_dir / \"layer4\" / \"tally.csv\"\n",
        "        if layer_tally.exists():\n",
        "            tally_path = layer_tally\n",
        "        else:\n",
        "            continue\n",
        "    epoch = int(epoch_dir.name.split(\"_\")[-1])\n",
        "    results.append((epoch, count_interpretable(tally_path, TAU)))\n",
        "\n",
        "results.sort()\n",
        "print(\"Epoch -> interpretable units (score >=\", TAU, \")\")\n",
        "for epoch, count in results:\n",
        "    print(epoch, \"->\", count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- PIPELINE: NETDISSECT DATA (BRODEN) ----\n",
        "%cd {REPO_DIR}/NetDissect-Lite\n",
        "!bash script/dlbroden.sh\n",
        "\n",
        "# Quick sanity check\n",
        "import os\n",
        "broden_index = \"dataset/broden1_224/index.csv\"\n",
        "print(\"Broden index exists:\", os.path.exists(broden_index))\n",
        "\n",
        "%cd {REPO_DIR}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- PIPELINE: NETDISSECT PER CHECKPOINT ----\n",
        "%cd {REPO_DIR}\n",
        "\n",
        "# Patch netdissect runner to force DATA_DIRECTORY + INDEX_FILE\n",
        "from pathlib import Path\n",
        "runner_path = Path(\"semantic_mortality/netdissect_runner.py\")\n",
        "if runner_path.exists():\n",
        "    text = runner_path.read_text()\n",
        "    if \"DATA_DIRECTORY\" not in text or \"INDEX_FILE\" not in text:\n",
        "        text = text.replace(\n",
        "            \"def _write_run_settings(\\n    settings_path: Path,\\n    *,\\n    model_file: Path,\\n    output_folder: Path,\\n    dataset: str,\\n) -> None:\",\n",
        "            \"def _write_run_settings(\\n    settings_path: Path,\\n    *,\\n    model_file: Path,\\n    output_folder: Path,\\n    dataset: str,\\n    data_directory: Path,\\n    index_file: str,\\n) -> None:\"\n",
        "        )\n",
        "        text = text.replace(\n",
        "            \"f\\\"DATASET = \\\\\\\"{dataset}\\\\\\\"\\\\n\\\"\\n\", \n",
        "            \"f\\\"DATASET = \\\\\\\"{dataset}\\\\\\\"\\\\n\\\"\\n        f\\\"DATA_DIRECTORY = r\\\\\\\"{data_directory.as_posix()}\\\\\\\"\\\\n\\\"\\n        f\\\"INDEX_FILE = \\\\\\\"{index_file}\\\\\\\"\\\\n\\\"\\n\"\n",
        "        )\n",
        "        text = text.replace(\n",
        "            \"_write_run_settings(\\n            run_settings,\\n            model_file=ckpt_path,\\n            output_folder=output_dir,\\n            dataset=\\\"places365\\\",\\n        )\",\n",
        "            \"broden_dir = net_root / \\\"dataset\\\" / \\\"broden1_224\\\"\\n        _write_run_settings(\\n            run_settings,\\n            model_file=ckpt_path,\\n            output_folder=output_dir,\\n            dataset=\\\"places365\\\",\\n            data_directory=broden_dir,\\n            index_file=\\\"index.csv\\\",\\n        )\"\n",
        "        )\n",
        "        runner_path.write_text(text)\n",
        "\n",
        "!python -m semantic_mortality.pipeline --stage dissect --config {CONFIG_PATH}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- PIPELINE: TRACKING + ANALYSIS + PLOTS ----\n",
        "%cd {REPO_DIR}\n",
        "!python -m semantic_mortality.pipeline --stage track --config {CONFIG_PATH}\n",
        "!python -m semantic_mortality.pipeline --stage analyze --config {CONFIG_PATH}\n",
        "!python -m semantic_mortality.pipeline --stage plot --config {CONFIG_PATH}\n",
        "\n",
        "print(\"Done. Check runs/places365_resnet18/analysis and runs/places365_resnet18/plots\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
